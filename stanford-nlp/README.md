# Stanford NLP Course Work

## Assignment 2

- [Assignment 2](https://github.com/study-groups/nlp-study-group/blob/master/projects/stanford-nlp/assignment2/assignment2.pdf): Assignment in Stanford NLP CS224n Spring 2017
  - **Softmax**
  
    In this section you'll implement a linear classifier, in Pytorch, with the following loss function:
    ```
    J(W) = CE(y,softmax(xW + b))
    ```
    Where CE refers to the cross-entropy loss function and Softmax refers the probability distribution that we want to maximize.
    
  - **Neural Transition-Based Dependency Parsing**
  
    A dependency parser analyzes the grammatical structure of a sentence. In this sections you'll implement a _transition-based_ parser, which incrimentally builds a parse one step at a time.
  - **Recurrent Neural Networks: Language Modeling**
  
    In this section you'll analyze a reccurent neural network (RNN) used for language modeling.
